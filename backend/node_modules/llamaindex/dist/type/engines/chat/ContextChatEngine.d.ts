import type { BaseChatEngine, NonStreamingChatEngineParams, StreamingChatEngineParams } from "@llamaindex/core/chat-engine";
import type { ChatMessage, LLM, MessageType } from "@llamaindex/core/llms";
import { BaseMemory } from "@llamaindex/core/memory";
import { type ContextSystemPrompt, type ModuleRecord, PromptMixin, type PromptsRecord } from "@llamaindex/core/prompts";
import type { BaseRetriever } from "@llamaindex/core/retriever";
import { EngineResponse } from "@llamaindex/core/schema";
import type { BaseNodePostprocessor } from "../../postprocessors/index.js";
import type { ContextGenerator } from "./types.js";
/**
 * ContextChatEngine uses the Index to get the appropriate context for each query.
 * The context is stored in the system prompt, and the chat history is chunk: ChatResponseChunk, nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[], nodes?: NodeWithScore<import("/Users/marcus/code/llamaindex/LlamaIndexTS/packages/core/src/Node").Metadata>[]lowing the appropriate context to be surfaced for each query.
 */
export declare class ContextChatEngine extends PromptMixin implements BaseChatEngine {
    chatModel: LLM;
    memory: BaseMemory;
    contextGenerator: ContextGenerator & PromptMixin;
    systemPrompt?: string | undefined;
    get chatHistory(): ChatMessage<object>[] | Promise<ChatMessage<object>[]>;
    constructor(init: {
        retriever: BaseRetriever;
        chatModel?: LLM | undefined;
        chatHistory?: ChatMessage[] | undefined;
        contextSystemPrompt?: ContextSystemPrompt | undefined;
        nodePostprocessors?: BaseNodePostprocessor[] | undefined;
        systemPrompt?: string | undefined;
        contextRole?: MessageType | undefined;
    });
    protected _getPrompts(): PromptsRecord;
    protected _updatePrompts(prompts: {
        contextSystemPrompt: ContextSystemPrompt;
    }): void;
    protected _getPromptModules(): ModuleRecord;
    chat(params: NonStreamingChatEngineParams): Promise<EngineResponse>;
    chat(params: StreamingChatEngineParams): Promise<AsyncIterable<EngineResponse>>;
    reset(): void;
    private prepareRequestMessages;
    private prependSystemPrompt;
}
