import { defaultContextSystemPrompt, PromptMixin } from "@llamaindex/core/prompts";
import { createMessageContent } from "@llamaindex/core/response-synthesizers";
import { MetadataMode } from "@llamaindex/core/schema";
export class DefaultContextGenerator extends PromptMixin {
    retriever;
    contextSystemPrompt;
    nodePostprocessors;
    contextRole;
    metadataMode;
    constructor(init){
        super();
        this.retriever = init.retriever;
        this.contextSystemPrompt = init?.contextSystemPrompt ?? defaultContextSystemPrompt;
        this.nodePostprocessors = init.nodePostprocessors || [];
        this.contextRole = init.contextRole ?? "system";
        this.metadataMode = init.metadataMode ?? MetadataMode.NONE;
    }
    _getPromptModules() {
        return {};
    }
    _getPrompts() {
        return {
            contextSystemPrompt: this.contextSystemPrompt
        };
    }
    _updatePrompts(promptsDict) {
        if (promptsDict.contextSystemPrompt) {
            this.contextSystemPrompt = promptsDict.contextSystemPrompt;
        }
    }
    async applyNodePostprocessors(nodes, query) {
        let nodesWithScore = nodes;
        for (const postprocessor of this.nodePostprocessors){
            nodesWithScore = await postprocessor.postprocessNodes(nodesWithScore, query);
        }
        return nodesWithScore;
    }
    async generate(message) {
        const sourceNodesWithScore = await this.retriever.retrieve({
            query: message
        });
        const nodes = await this.applyNodePostprocessors(sourceNodesWithScore, message);
        const content = await createMessageContent(this.contextSystemPrompt, nodes.map((r)=>r.node), undefined, this.metadataMode);
        return {
            message: {
                content,
                role: this.contextRole
            },
            nodes
        };
    }
}
